import argparse
import os
import sys
from pathlib import Path

from dotenv import load_dotenv

from langchain_community.vectorstores import FAISS
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

from langchain_openai import ChatOpenAI
from embeddings import get_embeddings
from src.reranker import CrossEncoderReranker
from langchain_community.retrievers import BM25Retriever
from langchain_classic.retrievers import EnsembleRetriever



load_dotenv(override=True)

DEFAULT_INDEX_DIR = Path(__file__).parent.parent / "data" / "index"

FIXED_INDEX_PATH = Path(__file__).parent.parent / "data" / "index" / "pis2116_single"

def load_vectorstore(index_path: Path):
    """åŠ è½½å·²ç»æ„å»ºå¥½çš„ FAISS å‘é‡åº“"""
    if not index_path.exists():
        raise FileNotFoundError(f"{index_path} does not exist.")

    embeddings = get_embeddings()
    vectorstore = FAISS.load_local(
        str(index_path),
        embeddings,
        allow_dangerous_deserialization=True)
    return vectorstore


def build_rag_chain(retriever, model):
    prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                "ä½ æ˜¯è½¦ä¼æ™ºèƒ½åº§èˆ±å¯¼èˆªå›¢é˜Ÿçš„å†…éƒ¨çŸ¥è¯†åº“åŠ©æ‰‹ï¼Œ"
                "è¯·ä¸¥æ ¼æ ¹æ®æä¾›çš„ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ï¼›"
                "å¦‚æœä¸Šä¸‹æ–‡é‡Œæ²¡æœ‰ç­”æ¡ˆï¼Œå°±ç›´è¯´ä¸çŸ¥é“ï¼Œä¸è¦çç¼–ã€‚\n\n"
                "ä¸Šä¸‹æ–‡ï¼š\n{context}"
            ),
            ("human", "{input}"),
        ]
    )
    rag_chain = (
        {
            "input": RunnablePassthrough(),
            "context": retriever,
        }
        | prompt
        | model
        | StrOutputParser()
    )
    return rag_chain


def rewrite_query(question: str) -> str:
    rewrite_model = ChatOpenAI(
        model="deepseek-chat",
        api_key=os.getenv("OPENAI_API_KEY"),
        base_url=os.getenv("OPENAI_API_BASE"),
        temperature=0.0,
    )
    rewrite_prompt = ChatPromptTemplate.from_template(
        "è¯·å°†ä¸‹é¢çš„ç”¨æˆ·é—®é¢˜æ”¹å†™æˆé€‚åˆåœ¨æŠ€æœ¯æ–‡æ¡£ä¸­æ£€ç´¢çš„ç®€çŸ­æŸ¥è¯¢è¯­å¥ï¼Œ"
        "ä¿ç•™å…³é”®ä¿¡æ¯ï¼Œåå‘å…³é”®è¯ï¼Œä¸è¦å®¢å¥—è¯ï¼Œç›´æ¥è¾“å‡ºæ”¹å†™ç»“æœï¼š\n\n"
        "ç”¨æˆ·é—®é¢˜ï¼š{question}"
    )

    chain = rewrite_prompt | rewrite_model | StrOutputParser()
    rewritten = chain.invoke({"question": question})
    return rewritten.strip()


def _build_arg_parser() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser(description="å•æ–‡æ¡£RAGæµ‹è¯•è„šæœ¬")
    parser.add_argument(
        "--question",
        "-q",
        type=str,
        help="ç›´æ¥ä¼ å…¥é—®é¢˜æ–‡æœ¬ï¼Œé¿å…äº¤äº’è¾“å…¥",
    )
    parser.add_argument(
        "--no-rewrite",
        action="store_true",
        help="è·³è¿‡ query rewriteï¼Œç›´æ¥ä½¿ç”¨åŸé—®é¢˜æ£€ç´¢",
    )
    parser.add_argument(
        "--top-k",
        type=int,
        default=4,
        help="æ£€ç´¢è¿”å›çš„æ–‡æ¡£ç‰‡æ®µæ•°é‡ï¼ˆé»˜è®¤ï¼š4ï¼‰",
    )
    parser.add_argument(
        "question_parts",
        nargs="*",
        help="å‘½ä»¤è¡Œç›´æ¥è¾“å…¥çš„é—®é¢˜å†…å®¹ï¼ˆå¯å«ç©ºæ ¼ï¼‰",
    )
    return parser


def _collect_question(args: argparse.Namespace) -> str:
    if args.question:
        return args.question.strip()
    if args.question_parts:
        return " ".join(args.question_parts).strip()
    return input("è¯·è¾“å…¥é—®é¢˜ï¼š").strip()


def main():
    parser = _build_arg_parser()
    args = parser.parse_args()

    question = _collect_question(args)
    if not question:
        print("è¾“å…¥ä¸ºç©ºï¼Œé€€å‡º")
        return

    index_path = FIXED_INDEX_PATH

    if args.no_rewrite:
        rewritten_query = question
        print("\nè·³è¿‡ query æ”¹å†™ï¼Œç›´æ¥ä½¿ç”¨åŸé—®é¢˜æ£€ç´¢ã€‚")
    else:
        print("\næ­£åœ¨æ”¹å†™æ£€ç´¢query...")
        rewritten_query = rewrite_query(question)
        print(f"åŸå§‹é—®é¢˜ï¼š{question}")
        print(f"æ£€ç´¢ç”¨æ”¹å†™ï¼š{rewritten_query}")

    print(f"\nğŸ“¦ æ­£åœ¨åŠ è½½å‘é‡åº“: {index_path}")
    vectorstore = load_vectorstore(index_path)

    # 1. ä»å‘é‡åº“é‡ŒæŠŠæ‰€æœ‰ Document æ‹¿å‡ºæ¥ï¼Œç»™ BM25 ç”¨
    all_docs = list(vectorstore.docstore._dict.values())
    print(f"å‘é‡åº“ä¸­å…±æœ‰æ–‡æ¡£å—: {len(all_docs)}")

    # 2. æ„å»º BM25 ç¨€ç–æ£€ç´¢å™¨ï¼ˆåŸºäºå€’æ’ç´¢å¼•ï¼Œå†…å­˜é‡Œå»ºå³å¯ï¼‰
    bm25_retriever = BM25Retriever.from_documents(all_docs)
    bm25_retriever.k = args.top_k * 3  # ç¨€ç–è¿™è¾¹å…ˆå¤šå–ä¸€ç‚¹å€™é€‰

    # 3. æ„å»ºåŸæ¥çš„ç¨ å¯†æ£€ç´¢å™¨ï¼ˆFAISSï¼‰
    dense_retriever = vectorstore.as_retriever(
        search_type="similarity",
        search_kwargs={"k": args.top_k * 3},  # å’Œ BM25 ç»Ÿä¸€ä¸€ä¸‹æ•°é‡
    )
    # 4. ç”¨ EnsembleRetriever æŠŠ BM25 + Dense èåˆæˆä¸€ä¸ªæ··åˆæ£€ç´¢å™¨
    hybrid_retriever = EnsembleRetriever(
        retrievers=[dense_retriever, bm25_retriever],
        weights=[0.7, 0.3],  # è¯­ä¹‰ 0.7 + å…³é”®è¯ 0.3ï¼Œåé¢å¯ä»¥å†è°ƒ
    )

    retriever = hybrid_retriever  # åé¢ç»Ÿä¸€ç”¨ retriever è¿™ä¸ªå˜é‡
    # æ£€ç´¢å¹¶æ˜¾ç¤ºæ–‡æ¡£ç‰‡æ®µ
    print(f"\nğŸ” æ­£åœ¨æ£€ç´¢ç›¸å…³æ–‡æ¡£ (top-{args.top_k * 3})...\n")
    # docs = retriever.invoke(rewritten_query)
    docs = retriever.invoke(question)
    reranker = CrossEncoderReranker()
    docs = reranker.rerank(question, docs,top_k=args.top_k)

    if not docs:
        print("æ²¡æœ‰æ‰¾åˆ°ç›¸å…³æ–‡æ¡£ï¼Œè¯·é‡æ–°æé—®ã€‚")
        return

    print("æ£€ç´¢åˆ°çš„æ–‡æ¡£ç‰‡æ®µï¼š")
    for i, d in enumerate(docs, 1):
        source = d.metadata.get("source", "æœªçŸ¥æ–‡ä»¶")
        page = d.metadata.get("page", "æ— é¡µç ")
        snippet = d.page_content[:300].replace("\n", " ")

        page_str = f"é¡µç ï¼š{page}" if page and page != "æ— é¡µç " else ""
        section = d.metadata.get("section", "")
        print(f"\n{i}. æ¥æºï¼š{source} {page_str}")
        if section:
            print(f"   ä½ç½®ï¼š{section}")
        print(f"   å†…å®¹ï¼š{d.page_content}...")
        print("-" * 80)

    print("\næ­£åœ¨ç”Ÿæˆå›ç­”(æµå¼è¾“å‡º)...\n")

    model = ChatOpenAI(
        model="deepseek-chat",
        api_key=os.getenv("OPENAI_API_KEY"),
        base_url=os.getenv("OPENAI_API_BASE"),
        temperature=0.3,
        streaming=True,
    )
    rag_chain = build_rag_chain(retriever, model)

    print(f"â“ é—®é¢˜ï¼š{question}\n")
    print("ğŸ’¬ å›ç­”ï¼š", end="", flush=True)

    # æµå¼è¾“å‡ºç­”æ¡ˆ
    for chunk in rag_chain.stream(question):
        print(chunk, end="", flush=True)

    print("\n")


if __name__ == "__main__":
    main()

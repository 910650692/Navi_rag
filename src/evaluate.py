# evaluate.py
"""
å¯¼èˆªçŸ¥è¯†åº“åŠ©æ‰‹ - RAG è¯„ä¼°è„šæœ¬ (RAGAS)
ä½¿ç”¨æ–¹æ³•ï¼š
    python evaluate.py

å…ˆå®‰è£…ä¾èµ–ï¼š
    pip install ragas datasets
"""

import json
import os
from pathlib import Path
from typing import List

from dotenv import load_dotenv
from datasets import Dataset

from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

from langchain_community.vectorstores import FAISS
from langchain_openai import ChatOpenAI

from ragas import evaluate
from ragas.metrics import (
    context_precision,
    context_recall,
    faithfulness,
    answer_relevancy,
    answer_correctness,
)

# ä½ è‡ªå·±çš„ embedding å°è£…
from src.embeddings import get_embeddings

load_dotenv(override=True)

BASE_DIR = Path(__file__).parent.parent
EVAL_FILE = BASE_DIR / "data" / "nav_rag_eval_set_v1.jsonl"
INDEX_PATH = BASE_DIR / "data" / "index" / "nav_faiss"


# =============== åŠ è½½å‘é‡åº“ & æ¨¡å‹ ===============

def load_vectorstore():
    if not INDEX_PATH.exists():
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°å‘é‡åº“ç›®å½•: {INDEX_PATH}")

    embeddings = get_embeddings()
    vectorstore = FAISS.load_local(
        str(INDEX_PATH),
        embeddings,
        allow_dangerous_deserialization=True,
    )
    return vectorstore


def get_llms():
    api_key = os.getenv("OPENAI_API_KEY")
    base_url = os.getenv("OPENAI_API_BASE")

    # ç”¨äº query rewrite
    rewrite_llm = ChatOpenAI(
        model="deepseek-chat",
        api_key=api_key,
        base_url=base_url,
        temperature=0.0,
        streaming=False,
    )

    # ç”¨äºæœ€ç»ˆå›ç­”ï¼ˆè¯„ä¼°æ—¶ä¸éœ€è¦æµå¼ï¼‰
    answer_llm = ChatOpenAI(
        model="deepseek-chat",
        api_key=api_key,
        base_url=base_url,
        temperature=0.3,
        streaming=False,
    )

    return rewrite_llm, answer_llm


# =============== Query Rewrite & RAG ===============

def rewrite_query(question: str, rewrite_llm: ChatOpenAI) -> str:
    prompt = ChatPromptTemplate.from_template(
        "ä½ æ˜¯ä¸€ä¸ªæ£€ç´¢åŠ©æ‰‹ï¼Œè¯·å°†ä¸‹é¢çš„ç”¨æˆ·é—®é¢˜æ”¹å†™æˆé€‚åˆåœ¨æŠ€æœ¯æ–‡æ¡£ä¸­æ£€ç´¢çš„ç®€çŸ­æŸ¥è¯¢è¯­å¥ï¼Œ"
        "ä¿ç•™å…³é”®ä¿¡æ¯ï¼Œå¯ä»¥é€‚å½“åŠ å…¥å¯èƒ½çš„åŒä¹‰è¯æˆ–ä¸“ä¸šæœ¯è¯­ï¼Œä¸è¦å®¢å¥—è¯ï¼Œç›´æ¥è¾“å‡ºæ”¹å†™ç»“æœï¼š\n\n"
        "ç”¨æˆ·é—®é¢˜ï¼š{question}"
    )
    chain = prompt | rewrite_llm | StrOutputParser()
    rewritten = chain.invoke({"question": question})
    return rewritten.strip()


def format_docs_for_prompt(docs: List[Document]) -> str:
    parts = []
    for i, d in enumerate(docs, 1):
        source = d.metadata.get("source", "æœªçŸ¥æ–‡ä»¶")
        page = d.metadata.get("page", None)
        header = f"[{i}] {source}"
        if page is not None:
            header += f" - é¡µç  {page}"
        parts.append(header + "\n" + d.page_content)
    return "\n\n".join(parts)


def build_rag_chain(answer_llm: ChatOpenAI):
    prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                "ä½ æ˜¯è½¦ä¼æ™ºèƒ½åº§èˆ±å¯¼èˆªå›¢é˜Ÿçš„å†…éƒ¨çŸ¥è¯†åº“åŠ©æ‰‹ï¼Œ"
                "è¯·ä¸¥æ ¼æ ¹æ®æä¾›çš„ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ï¼›"
                "å¦‚æœä¸Šä¸‹æ–‡é‡Œæ²¡æœ‰ç­”æ¡ˆï¼Œå°±ç›´è¯´ä¸çŸ¥é“ï¼Œä¸è¦çç¼–ã€‚\n\n"
                "ä¸Šä¸‹æ–‡ï¼š\n{context}",
            ),
            ("human", "{question}"),
        ]
    )

    rag_chain = (
        {
            "question": RunnablePassthrough(),
            "context": lambda x: x["context"],
        }
        | prompt
        | answer_llm
        | StrOutputParser()
    )

    return rag_chain


# =============== è¯»å– Eval Set ===============

def load_eval_items(path: Path):
    if not path.exists():
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°è¯„ä¼°é›†æ–‡ä»¶: {path}")

    items = []
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            if line.strip():
                items.append(json.loads(line))
    return items


# =============== ä¸»æµç¨‹ï¼šè·‘ RAG + RAGAS ===============

def main():
    print(f"ğŸ“„ æ­£åœ¨åŠ è½½è¯„ä¼°é›†: {EVAL_FILE}")
    eval_items = load_eval_items(EVAL_FILE)
    print(f"âœ… æ ·æœ¬æ•°é‡: {len(eval_items)}\n")

    print("ğŸ“¦ åŠ è½½å‘é‡åº“å’Œæ¨¡å‹...")
    vectorstore = load_vectorstore()
    retriever = vectorstore.as_retriever(
        search_type="mmr",
        search_kwargs={"k": 4, "fetch_k": 20},
    )
    rewrite_llm, answer_llm = get_llms()
    rag_chain = build_rag_chain(answer_llm)

    questions = []
    answers = []
    contexts_list = []
    ground_truths = []
    difficulties = []
    ids = []
    source_hints = []

    for idx, item in enumerate(eval_items, start=1):
        qid = item.get("id", f"Q{idx}")
        question = item["question"]
        gt = item["ground_truth"]
        difficulty = item.get("difficulty", "unknown")
        source_hint = item.get("source_hint", "")

        print(f"\n===== [{idx}/{len(eval_items)}] {qid} =====")
        print(f"â“ é—®é¢˜ï¼š{question}")
        print(f"ğŸ¯ éš¾åº¦ï¼š{difficulty} | æ¥æºæç¤ºï¼š{source_hint}")

        # 1. æ”¹å†™ Query
        rewritten = rewrite_query(question, rewrite_llm)
        print(f"âœï¸ æ”¹å†™åçš„æ£€ç´¢ Queryï¼š{rewritten}")

        # 2. æ£€ç´¢
        docs = retriever.invoke(rewritten)
        if not docs:
            print("âš ï¸ æœªæ£€ç´¢åˆ°ä»»ä½•æ–‡æ¡£ï¼Œä¸Šä¸‹æ–‡ç•™ç©ºã€‚")
            ctx_texts = []
            prompt_context = ""
        else:
            ctx_texts = [d.page_content for d in docs]
            prompt_context = format_docs_for_prompt(docs)
            print(f"ğŸ“˜ æ£€ç´¢åˆ° {len(docs)} ä¸ªæ–‡æ¡£ç‰‡æ®µã€‚")

        # 3. ç”Ÿæˆå›ç­”
        answer = rag_chain.invoke(
            {
                "question": question,
                "context": prompt_context,
            }
        )
        print(f"ğŸ’¬ å›ç­”ï¼š{answer[:200]}{'...' if len(answer) > 200 else ''}")

        # 4. æ”¶é›†æ•°æ®
        ids.append(qid)
        questions.append(question)
        answers.append(answer)
        contexts_list.append(ctx_texts)          # list[str]
        ground_truths.append(gt)                # ragas éœ€è¦ ground truth å­—ç¬¦ä¸²
        difficulties.append(difficulty)
        source_hints.append(source_hint)

    # æ„å»º HF Dataset
    print("\nğŸ“Š æ„å»º RAGAS æ•°æ®é›†...")
    ds = Dataset.from_dict(
        {
            "id": ids,
            "question": questions,
            "answer": answers,
            "contexts": contexts_list,
            "ground_truth": ground_truths,
            "difficulty": difficulties,
            "source_hint": source_hints,
        }
    )

    print("âœ… æ•°æ®é›†æ„å»ºå®Œæˆï¼Œå¼€å§‹è°ƒç”¨ RAGAS è¯„ä¼°...\n")

    # æ³¨æ„ï¼šè¿™é‡Œç›´æ¥æŠŠ answer_llm å’Œ embeddings ä¼ ç»™ ragas åšæ‰“åˆ†
    embeddings = get_embeddings()

    results = evaluate(
        ds,
        metrics=[
            context_precision,
            context_recall,
            faithfulness,
            answer_relevancy,
            answer_correctness,
        ],
        llm=answer_llm,
        embeddings=embeddings,
    )

    print("ğŸ“ˆ è¯„ä¼°ç»“æœï¼š")
    print(results)

    # å¯¼å‡ºä¸º csv æ–¹ä¾¿æŸ¥çœ‹
    out_path = BASE_DIR / "ragas_results.csv"
    results.to_pandas().to_csv(out_path, index=False, encoding="utf-8-sig")
    print(f"\nâœ… è¯¦ç»†è¯„ä¼°ç»“æœå·²å¯¼å‡ºåˆ°: {out_path}")


if __name__ == "__main__":
    main()

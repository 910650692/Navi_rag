"""
å¯¼èˆªçŸ¥è¯†åº“åŠ©æ‰‹ - RAG è¯„ä¼°è„šæœ¬ (RAGAS)
ä½¿ç”¨æ–¹æ³•ï¼š
    python evaluate.py

å…ˆå®‰è£…ä¾èµ–ï¼š
    pip install ragas datasets
"""

import argparse
import json
import os
from pathlib import Path
from typing import Any, Dict, List, Sequence

from dotenv import load_dotenv
from datasets import Dataset

from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

from langchain_community.vectorstores import FAISS
from langchain_community.retrievers import BM25Retriever
from langchain_classic.retrievers import EnsembleRetriever
from langchain_openai import ChatOpenAI

from ragas import evaluate
from ragas.metrics import (
    context_precision,
    context_recall,
    faithfulness,
    answer_relevancy,
    answer_correctness,
)

# ä½ è‡ªå·±çš„ embedding å°è£…
from embeddings import get_embeddings
from reranker import CrossEncoderReranker
from eval_utils import get_eval_recorder

load_dotenv(override=True)

BASE_DIR = Path(__file__).parent.parent

# è¯„ä¼°é›†æ–‡ä»¶ï¼šä½¿ç”¨åˆšæ‰ç”Ÿæˆçš„ QA jsonl
EVAL_FILE = BASE_DIR / "data" / "nav_rag_eval_set_v1.jsonl"

# å‘é‡åº“è·¯å¾„ï¼šä½ å¯ä»¥æ ¹æ®è‡ªå·±çš„é¡¹ç›®è°ƒæ•´
INDEX_PATH = BASE_DIR / "data" / "index" / "pis2116_single"


RAGAS_METRIC_COLUMNS = [
    "context_precision",
    "context_recall",
    "faithfulness",
    "answer_relevancy",
    "answer_correctness",
]


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description="å¯¹ç…§è¯„ä¼° Dense / Hybrid æ£€ç´¢é“¾è·¯çš„ RAG è¡¨ç°ï¼ˆåŸºäº RAGASï¼‰ã€‚"
    )
    parser.add_argument(
        "--mode",
        choices=["dense", "hybrid", "both"],
        default="both",
        help="é€‰æ‹©è¯„ä¼°å“ªä¸€ç§æ£€ç´¢æ¨¡å¼ï¼›both ä¼šä¾æ¬¡è¾“å‡º dense/hybrid ä¸¤ä»½ CSVã€‚",
    )
    parser.add_argument(
        "--top-k",
        type=int,
        default=4,
        help="æœ€ç»ˆå–‚ç»™ç”Ÿæˆæ¨¡å‹çš„ä¸Šä¸‹æ–‡æ•°é‡ï¼ˆé»˜è®¤ 4ï¼‰ã€‚",
    )
    parser.add_argument(
        "--candidate-multiplier",
        type=int,
        default=3,
        help="æ£€ç´¢å€™é€‰æ•°é‡ = top_k * å€æ•°ï¼Œç”¨äº rerank/æ··åˆèåˆï¼ˆè‡³å°‘ 1ï¼‰ã€‚",
    )
    parser.add_argument(
        "--output-dir",
        type=Path,
        default=BASE_DIR,
        help="è¯„ä¼°ç»“æœ CSV å­˜æ”¾ç›®å½•ï¼ˆé»˜è®¤é¡¹ç›®æ ¹ç›®å½•ï¼‰ã€‚",
    )
    parser.add_argument(
        "--dense-weight",
        type=float,
        default=0.7,
        help="Hybrid æ¨¡å¼ä¸­ç¨ å¯†æ£€ç´¢å¾—åˆ†æƒé‡ã€‚",
    )
    parser.add_argument(
        "--bm25-weight",
        type=float,
        default=0.3,
        help="Hybrid æ¨¡å¼ä¸­ BM25 æ£€ç´¢å¾—åˆ†æƒé‡ã€‚",
    )
    parser.add_argument(
        "--disable-reranker",
        action="store_true",
        help="å…³é—­ CrossEncoder rerankerï¼ˆé»˜è®¤å¼€å¯ï¼‰ã€‚",
    )
    parser.add_argument(
        "--no-query-rewrite",
        action="store_true",
        help="è·³è¿‡ Query Rewriteï¼Œç›´æ¥ä½¿ç”¨åŸå§‹é—®é¢˜æ£€ç´¢ã€‚",
    )
    return parser.parse_args()



# =============== åŠ è½½å‘é‡åº“ & æ¨¡å‹ ===============

def load_vectorstore():
    if not INDEX_PATH.exists():
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°å‘é‡åº“ç›®å½•: {INDEX_PATH}")

    embeddings = get_embeddings()
    vectorstore = FAISS.load_local(
        str(INDEX_PATH),
        embeddings,
        allow_dangerous_deserialization=True,
    )
    return vectorstore


def get_llms():
    api_key = os.getenv("OPENAI_API_KEY")
    base_url = os.getenv("OPENAI_API_BASE")

    # ç”¨äº query rewrite
    rewrite_llm = ChatOpenAI(
        model="deepseek-chat",
        api_key=api_key,
        base_url=base_url,
        temperature=0.0,
        streaming=False,
    )

    # ç”¨äºæœ€ç»ˆå›ç­”ï¼ˆè¯„ä¼°æ—¶ä¸éœ€è¦æµå¼ï¼‰
    answer_llm = ChatOpenAI(
        model="deepseek-chat",
        api_key=api_key,
        base_url=base_url,
        temperature=0.1,
        streaming=False,
    )

    return rewrite_llm, answer_llm


# =============== Query Rewrite & RAG ===============

def rewrite_query(question: str, rewrite_llm: ChatOpenAI) -> str:
    prompt = ChatPromptTemplate.from_template(
        "ä½ æ˜¯ä¸€ä¸ªæ£€ç´¢åŠ©æ‰‹ï¼Œè¯·å°†ä¸‹é¢çš„ç”¨æˆ·é—®é¢˜æ”¹å†™æˆé€‚åˆåœ¨æŠ€æœ¯æ–‡æ¡£ä¸­æ£€ç´¢çš„ç®€çŸ­æŸ¥è¯¢è¯­å¥ï¼Œ"
        "ä¿ç•™å…³é”®ä¿¡æ¯ï¼Œå¯ä»¥é€‚å½“åŠ å…¥å¯èƒ½çš„åŒä¹‰è¯æˆ–ä¸“ä¸šæœ¯è¯­ï¼Œä¸è¦å®¢å¥—è¯ï¼Œç›´æ¥è¾“å‡ºæ”¹å†™ç»“æœï¼š\n\n"
        "ç”¨æˆ·é—®é¢˜ï¼š{question}"
    )
    chain = prompt | rewrite_llm | StrOutputParser()
    rewritten = chain.invoke({"question": question})
    return rewritten.strip()


def format_docs_for_prompt(docs: List[Document]) -> str:
    parts = []
    for i, d in enumerate(docs, 1):
        source = d.metadata.get("source", "æœªçŸ¥æ–‡ä»¶")
        page = d.metadata.get("page", None)
        header = f"[{i}] {source}"
        if page is not None:
            header += f" - é¡µç  {page}"
        parts.append(header + "\n" + d.page_content)
    return "\n\n".join(parts)


def build_rag_chain(answer_llm: ChatOpenAI):
    prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                "ä½ æ˜¯è½¦ä¼æ™ºèƒ½åº§èˆ±å¯¼èˆªå›¢é˜Ÿçš„å†…éƒ¨çŸ¥è¯†åº“åŠ©æ‰‹ã€‚\n"
                "ã€éå¸¸é‡è¦ã€‘\n"
                "1. åªèƒ½ä¾æ®â€œä¸Šä¸‹æ–‡â€ä¸­çš„å†…å®¹å›ç­”ï¼Œä¸è¦åŠ å…¥ä»»ä½•ä¸Šä¸‹æ–‡ä¹‹å¤–çš„æ¨æµ‹ã€‚\n"
                "2. å¿…é¡»ç´§æ‰£ç”¨æˆ·é—®é¢˜ä½œç­”ï¼Œä¸è¦è¾“å‡ºä¸é—®é¢˜æ— å…³çš„è§£é‡Šæˆ–èƒŒæ™¯ã€‚\n"
                "3. å¦‚æœé—®é¢˜åœ¨ä¸Šä¸‹æ–‡ä¸­æœ‰æ˜ç¡®çš„æšä¸¾æˆ–åˆ—è¡¨ï¼Œè¯·å°½é‡å®Œæ•´åˆ—å‡ºï¼Œä¸è¦éšæ„çœç•¥ã€‚\n"
                "4. å¦‚æœä¸Šä¸‹æ–‡æ²¡æœ‰è¶³å¤Ÿä¿¡æ¯ï¼Œè¯·å›ç­”â€œæ ¹æ®å½“å‰æ–‡æ¡£ä¿¡æ¯æ— æ³•ç¡®å®šâ€ã€‚\n\n"
                "ä¸‹é¢æ˜¯æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ï¼š\n{context}"
            ),
            ("human", "{question}"),
        ]
    )

    rag_chain = (
        {
            "question": RunnablePassthrough(),
            "context": lambda x: x["context"],
        }
        | prompt
        | answer_llm
        | StrOutputParser()
    )

    return rag_chain


def _resolve_modes(mode_arg: str) -> List[str]:
    return ["dense", "hybrid"] if mode_arg == "both" else [mode_arg]


def _compute_fetch_k(top_k: int, multiplier: int) -> int:
    multiplier = max(1, multiplier)
    return max(top_k, top_k * multiplier)


def build_retriever(
    vectorstore: FAISS,
    mode: str,
    fetch_k: int,
    dense_weight: float,
    bm25_weight: float,
):
    dense_retriever = vectorstore.as_retriever(
        search_type="similarity",
        search_kwargs={"k": fetch_k},
    )

    if mode == "dense":
        return dense_retriever

    all_docs = list(getattr(vectorstore.docstore, "_dict", {}).values())
    if not all_docs:
        raise ValueError("å‘é‡åº“ docstore ä¸ºç©ºï¼Œæ— æ³•æ„å»º BM25 æ£€ç´¢å™¨ã€‚")

    bm25_retriever = BM25Retriever.from_documents(all_docs)
    bm25_retriever.k = fetch_k
    weights = [dense_weight, bm25_weight]
    return EnsembleRetriever(retrievers=[dense_retriever, bm25_retriever], weights=weights)


def _get_rewritten_query(
    question: str,
    rewrite_llm: ChatOpenAI,
    cache: Dict[str, str],
    skip_rewrite: bool,
) -> tuple[str, bool]:
    if skip_rewrite:
        return question, False
    if question in cache:
        return cache[question], False
    rewritten = rewrite_query(question, rewrite_llm)
    cache[question] = rewritten
    return rewritten, True


# =============== è¯»å– Eval Set ===============

def load_eval_items(path: Path):
    if not path.exists():
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°è¯„ä¼°é›†æ–‡ä»¶: {path}")

    items = []
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            if line.strip():
                items.append(json.loads(line))
    return items


# =============== ä¸»æµç¨‹ï¼šè·‘ RAG + RAGAS ===============


def run_mode_evaluation(
    mode: str,
    eval_items: Sequence[dict],
    vectorstore: FAISS,
    rewrite_llm: ChatOpenAI,
    rag_chain,
    answer_llm: ChatOpenAI,
    ragas_embeddings,
    args: argparse.Namespace,
    rewrite_cache: Dict[str, str],
    use_reranker: bool,
) -> tuple[Any, Path]:
    fetch_k = _compute_fetch_k(args.top_k, args.candidate_multiplier)
    retriever = build_retriever(
        vectorstore=vectorstore,
        mode=mode,
        fetch_k=fetch_k,
        dense_weight=args.dense_weight,
        bm25_weight=args.bm25_weight,
    )
    reranker = CrossEncoderReranker() if use_reranker else None

    print(f"\n{'=' * 15} å¼€å§‹è¯„ä¼°æ¨¡å¼ï¼š{mode.upper()} {'=' * 15}")
    print(f"ğŸ¯ å€™é€‰æ•°é‡: {fetch_k} | æœ€ç»ˆ top-k: {args.top_k} | reranker: {'å¼€å¯' if reranker else 'å…³é—­'}")

    questions: List[str] = []
    answers: List[str] = []
    contexts_list: List[List[str]] = []
    ground_truths: List[str] = []
    difficulties: List[str] = []
    ids: List[str] = []
    source_hints: List[str] = []

    for idx, item in enumerate(eval_items, start=1):
        qid = item.get('id', f"Q{idx}")
        question = item['question']
        gt = item['ground_truth']
        difficulty = item.get('difficulty', 'unknown')
        source_hint = item.get('source_hint', '')

        print(f"\n----- [{idx}/{len(eval_items)}] {qid} ({mode}) -----")
        print(f"â“ é—®é¢˜ï¼š{question}")
        print(f"ğŸ¯ éš¾åº¦ï¼š{difficulty} | æ¥æºæç¤ºï¼š{source_hint}")

        rewritten, freshly_computed = _get_rewritten_query(
            question, rewrite_llm, rewrite_cache, args.no_query_rewrite
        )
        if args.no_query_rewrite:
            print("âœï¸ å·²ç¦ç”¨ Query Rewriteï¼Œç›´æ¥ä½¿ç”¨åŸé—®é¢˜ã€‚")
        else:
            prefix = "âœï¸ æ–°æ”¹å†™" if freshly_computed else "â™»ï¸ ä½¿ç”¨ç¼“å­˜æ”¹å†™"
            print(f"{prefix}ï¼š{rewritten}")

        docs = retriever.invoke(rewritten)
        if not docs:
            print("âš ï¸ æœªæ£€ç´¢åˆ°ä»»ä½•æ–‡æ¡£ï¼Œä¸Šä¸‹æ–‡ç•™ç©ºã€‚")
            filtered_docs: List[Document] = []
        else:
            if reranker:
                filtered_docs = reranker.rerank(question, docs, top_k=args.top_k)
            else:
                filtered_docs = docs[: args.top_k]
            print(f"ğŸ” æ£€ç´¢å€™é€‰ {len(docs)} -> é€‰å– {len(filtered_docs)} æ¡ç”¨äºå›ç­”ã€‚")

        ctx_texts = [d.page_content for d in filtered_docs]
        prompt_context = format_docs_for_prompt(filtered_docs) if filtered_docs else ''

        answer = rag_chain.invoke(
            {
                'question': question,
                'context': prompt_context,
            }
        )
        print(f"ğŸ’¬ å›ç­”ï¼š{answer[:200]}{'...' if len(answer) > 200 else ''}")

        ids.append(qid)
        questions.append(question)
        answers.append(answer)
        contexts_list.append(ctx_texts)
        ground_truths.append(gt[0] if isinstance(gt, list) else gt)
        difficulties.append(difficulty)
        source_hints.append(source_hint)

    print("\nğŸ“Š æ„å»º RAGAS æ•°æ®é›†...")
    ds = Dataset.from_dict(
        {
            'id': ids,
            'question': questions,
            'answer': answers,
            'contexts': contexts_list,
            'ground_truth': ground_truths,
            'difficulty': difficulties,
            'source_hint': source_hints,
        }
    )

    print("âœ… æ•°æ®é›†æ„å»ºå®Œæˆï¼Œè°ƒç”¨ RAGAS è¯„ä¼°...\n")

    results = evaluate(
        ds,
        metrics=[
            context_precision,
            context_recall,
            faithfulness,
            answer_relevancy,
            answer_correctness,
        ],
        llm=answer_llm,
        embeddings=ragas_embeddings,
    )

    out_path = args.output_dir / f"ragas_results_{mode}.csv"
    df = results.to_pandas()
    df.to_csv(out_path, index=False, encoding='utf-8-sig')
    print(f"ğŸ“ˆ æ¨¡å¼ {mode} è¯„ä¼°å®Œæˆï¼Œç»“æœå†™å…¥ {out_path}\n")
    return df, out_path


def summarize_metrics(df) -> Dict[str, float]:
    summary = {}
    for col in RAGAS_METRIC_COLUMNS:
        if col in df.columns:
            summary[col] = float(df[col].mean())
    return summary


def main():
    args = parse_args()
    args.output_dir = args.output_dir.resolve()
    args.output_dir.mkdir(parents=True, exist_ok=True)
    use_reranker = not args.disable_reranker

    print(f"ğŸ“„ æ­£åœ¨åŠ è½½è¯„ä¼°é›†: {EVAL_FILE}")
    eval_items = load_eval_items(EVAL_FILE)
    print(f"âœ… æ ·æœ¬æ•°é‡: {len(eval_items)}")

    print("ğŸ“¦ åŠ è½½å‘é‡åº“å’Œæ¨¡å‹...")
    vectorstore = load_vectorstore()
    rewrite_llm, answer_llm = get_llms()
    rag_chain = build_rag_chain(answer_llm)
    ragas_embeddings = get_embeddings()

    rewrite_cache: Dict[str, str] = {}
    modes = _resolve_modes(args.mode)
    summary_rows = []

    for mode in modes:
        df, out_path = run_mode_evaluation(
            mode=mode,
            eval_items=eval_items,
            vectorstore=vectorstore,
            rewrite_llm=rewrite_llm,
            rag_chain=rag_chain,
            answer_llm=answer_llm,
            ragas_embeddings=ragas_embeddings,
            args=args,
            rewrite_cache=rewrite_cache,
            use_reranker=use_reranker,
        )
        summary = summarize_metrics(df)
        summary_rows.append((mode, summary, out_path))

    print("\n====== æ±‡æ€»å¯¹æ¯” ======")
    recorder = get_eval_recorder()

    for mode, summary, path in summary_rows:
        metric_parts = [
            f"{metric}: {summary.get(metric, float('nan')):.3f}"
            for metric in RAGAS_METRIC_COLUMNS
            if metric in summary
        ]
        print(f"{mode.upper()} -> {', '.join(metric_parts)} | CSV: {path}")

        # ä¿å­˜è¯„ä¼°ç»“æœåˆ°JSONL
        config = {
            "retriever": mode,
            "use_reranker": use_reranker,
            "top_k": args.top_k,
            "candidate_k": _compute_fetch_k(args.top_k, args.candidate_multiplier),
            "dense_weight": args.dense_weight if mode == "hybrid" else None,
            "bm25_weight": args.bm25_weight if mode == "hybrid" else None,
            "query_rewrite": not args.no_query_rewrite,
        }

        notes = f"è¯„ä¼°é›†: {EVAL_FILE.name}, æ ·æœ¬æ•°: {len(eval_items)}"

        eval_id = recorder.save_eval_result(config=config, metrics=summary, notes=notes)
        print(f"   âœ… è¯„ä¼°ç»“æœå·²ä¿å­˜ï¼ŒID: {eval_id}\n")


if __name__ == '__main__':
    main()


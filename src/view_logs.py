"""
RAGæ—¥å¿—æŸ¥çœ‹å·¥å…·
ç”¨äºŽæŸ¥çœ‹å’Œåˆ†æžRAGæŸ¥è¯¢æ—¥å¿—
"""

import json
from pathlib import Path
from datetime import datetime
from collections import Counter


def view_logs(log_file: str = None, limit: int = 10):
    """
    æŸ¥çœ‹æœ€è¿‘çš„RAGæŸ¥è¯¢æ—¥å¿—

    Args:
        log_file: æ—¥å¿—æ–‡ä»¶è·¯å¾„ï¼ˆå¦‚æžœä¸æŒ‡å®šï¼ŒæŸ¥çœ‹æœ€æ–°çš„æ—¥å¿—æ–‡ä»¶ï¼‰
        limit: æ˜¾ç¤ºæœ€è¿‘çš„Næ¡è®°å½•
    """
    log_dir = Path(__file__).parent.parent / "data" / "logs"

    if log_file:
        log_path = Path(log_file)
    else:
        # æ‰¾æœ€æ–°çš„æ—¥å¿—æ–‡ä»¶
        log_files = list(log_dir.glob("rag_queries_*.jsonl"))
        if not log_files:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°æ—¥å¿—æ–‡ä»¶")
            return

        log_path = max(log_files, key=lambda p: p.stat().st_mtime)

    print("=" * 80)
    print(f"ðŸ“‹ æ—¥å¿—æ–‡ä»¶: {log_path.name}")
    print("=" * 80)

    # è¯»å–æ—¥å¿—
    logs = []
    with open(log_path, 'r', encoding='utf-8') as f:
        for line in f:
            try:
                logs.append(json.loads(line))
            except json.JSONDecodeError:
                continue

    if not logs:
        print("âš ï¸  æ—¥å¿—æ–‡ä»¶ä¸ºç©º")
        return

    print(f"\næ€»æŸ¥è¯¢æ¬¡æ•°: {len(logs)}")
    print(f"æ˜¾ç¤ºæœ€è¿‘ {min(limit, len(logs))} æ¡è®°å½•:\n")

    # æ˜¾ç¤ºæœ€è¿‘Næ¡
    for i, log in enumerate(logs[-limit:], 1):
        print(f"\n{'='*80}")
        print(f"æŸ¥è¯¢ #{len(logs) - limit + i}")
        print(f"{'='*80}")

        timestamp = datetime.fromisoformat(log['timestamp']).strftime("%Y-%m-%d %H:%M:%S")
        print(f"æ—¶é—´: {timestamp}")
        print(f"Session: {log['session_id']}")
        print(f"\nåŽŸå§‹é—®é¢˜: {log['query']}")

        if log.get('rewritten_query'):
            print(f"æ”¹å†™é—®é¢˜: {log['rewritten_query']}")

        print(f"\né…ç½®:")
        print(f"  - Queryæ”¹å†™: {'âœ“' if log['use_rewriter'] else 'âœ—'}")
        print(f"  - é‡æŽ’åº: {'âœ“' if log['use_reranker'] else 'âœ—'}")
        print(f"  - Top-K: {log['top_k']}")

        print(f"\næ£€ç´¢ç»“æžœ: {len(log['retrieval_docs'])} ä¸ªæ–‡æ¡£")
        for doc in log['retrieval_docs'][:3]:  # åªæ˜¾ç¤ºå‰3ä¸ª
            source = doc.get('source', 'æœªçŸ¥')
            rank = doc.get('rank', '?')
            print(f"  [{rank}] {source}", end="")

            if 'section' in doc:
                print(f" - {doc['section']}", end="")
            elif 'row_number' in doc:
                print(f" - è¡Œ{doc['row_number']}", end="")
            elif 'page' in doc:
                print(f" - é¡µ{doc['page']}", end="")

            print()

        if len(log['retrieval_docs']) > 3:
            print(f"  ... è¿˜æœ‰ {len(log['retrieval_docs']) - 3} ä¸ªæ–‡æ¡£")

        print(f"\nç­”æ¡ˆé•¿åº¦: {log['answer_length']} å­—ç¬¦")
        print(f"ç­”æ¡ˆé¢„è§ˆ: {log['answer'][:100]}...")

        print(f"\næ€§èƒ½:")
        print(f"  - æ€»è€—æ—¶: {log['latency_ms']:.0f} ms")
        if log.get('retrieval_latency_ms'):
            print(f"  - æ£€ç´¢è€—æ—¶: {log['retrieval_latency_ms']:.0f} ms")
        if log.get('llm_latency_ms'):
            print(f"  - LLMè€—æ—¶: {log['llm_latency_ms']:.0f} ms")

        if log.get('error'):
            print(f"\nâŒ é”™è¯¯: [{log.get('error_type')}] {log['error']}")


def analyze_logs(log_file: str = None):
    """
    åˆ†æžRAGæŸ¥è¯¢æ—¥å¿—ï¼Œç”Ÿæˆç»Ÿè®¡ä¿¡æ¯

    Args:
        log_file: æ—¥å¿—æ–‡ä»¶è·¯å¾„ï¼ˆå¦‚æžœä¸æŒ‡å®šï¼ŒæŸ¥çœ‹æœ€æ–°çš„æ—¥å¿—æ–‡ä»¶ï¼‰
    """
    log_dir = Path(__file__).parent.parent / "data" / "logs"

    if log_file:
        log_path = Path(log_file)
    else:
        # æ‰¾æœ€æ–°çš„æ—¥å¿—æ–‡ä»¶
        log_files = list(log_dir.glob("rag_queries_*.jsonl"))
        if not log_files:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°æ—¥å¿—æ–‡ä»¶")
            return

        log_path = max(log_files, key=lambda p: p.stat().st_mtime)

    print("=" * 80)
    print(f"ðŸ“Š æ—¥å¿—åˆ†æž: {log_path.name}")
    print("=" * 80)

    # è¯»å–æ—¥å¿—
    logs = []
    with open(log_path, 'r', encoding='utf-8') as f:
        for line in f:
            try:
                logs.append(json.loads(line))
            except json.JSONDecodeError:
                continue

    if not logs:
        print("âš ï¸  æ—¥å¿—æ–‡ä»¶ä¸ºç©º")
        return

    print(f"\nðŸ“ˆ åŸºæœ¬ç»Ÿè®¡:")
    print(f"  æ€»æŸ¥è¯¢æ¬¡æ•°: {len(logs)}")

    # é…ç½®ç»Ÿè®¡
    rewriter_count = sum(1 for log in logs if log['use_rewriter'])
    reranker_count = sum(1 for log in logs if log['use_reranker'])
    print(f"\nâš™ï¸  é…ç½®ä½¿ç”¨:")
    print(f"  Queryæ”¹å†™: {rewriter_count}/{len(logs)} ({rewriter_count/len(logs)*100:.1f}%)")
    print(f"  é‡æŽ’åº: {reranker_count}/{len(logs)} ({reranker_count/len(logs)*100:.1f}%)")

    # æ€§èƒ½ç»Ÿè®¡
    latencies = [log['latency_ms'] for log in logs if log['latency_ms']]
    if latencies:
        print(f"\nâ±ï¸  æ€§èƒ½ç»Ÿè®¡:")
        print(f"  å¹³å‡è€—æ—¶: {sum(latencies)/len(latencies):.0f} ms")
        print(f"  æœ€å¿«: {min(latencies):.0f} ms")
        print(f"  æœ€æ…¢: {max(latencies):.0f} ms")

    # æ–‡æ¡£æ¥æºç»Ÿè®¡
    all_sources = []
    for log in logs:
        for doc in log['retrieval_docs']:
            all_sources.append(doc.get('source', 'æœªçŸ¥'))

    source_counter = Counter(all_sources)
    print(f"\nðŸ“š æ£€ç´¢æ¥æºç»Ÿè®¡ (Top 5):")
    for source, count in source_counter.most_common(5):
        print(f"  {source}: {count} æ¬¡")

    # é”™è¯¯ç»Ÿè®¡
    errors = [log for log in logs if log.get('error')]
    if errors:
        print(f"\nâŒ é”™è¯¯ç»Ÿè®¡:")
        print(f"  é”™è¯¯æ¬¡æ•°: {len(errors)}/{len(logs)} ({len(errors)/len(logs)*100:.1f}%)")

        error_types = Counter(log.get('error_type') for log in errors)
        for error_type, count in error_types.most_common():
            print(f"  {error_type}: {count} æ¬¡")


if __name__ == "__main__":
    import sys

    if len(sys.argv) > 1 and sys.argv[1] == "--analyze":
        analyze_logs()
    else:
        limit = int(sys.argv[1]) if len(sys.argv) > 1 else 10
        view_logs(limit=limit)

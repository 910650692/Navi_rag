"""
å¯¼èˆªçŸ¥è¯†åº“åŠ©æ‰‹ - æ··åˆæ£€ç´¢ RAG è¯„ä¼°è„šæœ¬ (ä»… hybrid æ¨¡å¼)
ä½¿ç”¨æ–¹æ³•ï¼š
    python evaluate_hybrid.py
"""

import json
import os
from pathlib import Path
from typing import Any, Dict, List

from dotenv import load_dotenv
from datasets import Dataset

from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

from langchain_community.vectorstores import FAISS
from langchain_community.retrievers import BM25Retriever
from langchain_classic.retrievers import EnsembleRetriever
from langchain_openai import ChatOpenAI

from ragas import evaluate
from ragas.metrics import (
    context_precision,
    context_recall,
    faithfulness,
    answer_relevancy,
    answer_correctness,
)

from embeddings import get_embeddings
from reranker import CrossEncoderReranker

load_dotenv(override=True)

BASE_DIR = Path(__file__).parent.parent
EVAL_FILE = BASE_DIR / "data" / "nav_rag_eval_set_v1.jsonl"
INDEX_PATH = BASE_DIR / "data" / "index" / "pis2116_single"

# æ··åˆæ£€ç´¢å‚æ•°
TOP_K = 4
CANDIDATE_MULTIPLIER = 3
DENSE_WEIGHT = 0.7
BM25_WEIGHT = 0.3
USE_RERANKER = True
SKIP_QUERY_REWRITE = False

RAGAS_METRIC_COLUMNS = [
    "context_precision",
    "context_recall",
    "faithfulness",
    "answer_relevancy",
    "answer_correctness",
]


def load_vectorstore():
    if not INDEX_PATH.exists():
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°å‘é‡åº“ç›®å½•: {INDEX_PATH}")
    embeddings = get_embeddings()
    vectorstore = FAISS.load_local(
        str(INDEX_PATH),
        embeddings,
        allow_dangerous_deserialization=True,
    )
    return vectorstore


def get_llms():
    api_key = os.getenv("OPENAI_API_KEY")
    base_url = os.getenv("OPENAI_API_BASE")

    rewrite_llm = ChatOpenAI(
        model="deepseek-chat",
        api_key=api_key,
        base_url=base_url,
        temperature=0.0,
        streaming=False,
    )

    answer_llm = ChatOpenAI(
        model="deepseek-chat",
        api_key=api_key,
        base_url=base_url,
        temperature=0.1,
        streaming=False,
    )

    return rewrite_llm, answer_llm


def rewrite_query(question: str, rewrite_llm: ChatOpenAI) -> str:
    prompt = ChatPromptTemplate.from_template(
        "ä½ æ˜¯ä¸€ä¸ªæ£€ç´¢åŠ©æ‰‹ï¼Œè¯·å°†ä¸‹é¢çš„ç”¨æˆ·é—®é¢˜æ”¹å†™æˆé€‚åˆåœ¨æŠ€æœ¯æ–‡æ¡£ä¸­æ£€ç´¢çš„ç®€çŸ­æŸ¥è¯¢è¯­å¥ï¼Œ"
        "ä¿ç•™å…³é”®ä¿¡æ¯ï¼Œå¯ä»¥é€‚å½“åŠ å…¥å¯èƒ½çš„åŒä¹‰è¯æˆ–ä¸“ä¸šæœ¯è¯­ï¼Œä¸è¦å®¢å¥—è¯ï¼Œç›´æ¥è¾“å‡ºæ”¹å†™ç»“æœï¼š\n\n"
        "ç”¨æˆ·é—®é¢˜ï¼š{question}"
    )
    chain = prompt | rewrite_llm | StrOutputParser()
    rewritten = chain.invoke({"question": question})
    return rewritten.strip()


def format_docs_for_prompt(docs: List[Document]) -> str:
    parts = []
    for i, d in enumerate(docs, 1):
        source = d.metadata.get("source", "æœªçŸ¥æ–‡ä»¶")
        page = d.metadata.get("page", None)
        header = f"[{i}] {source}"
        if page is not None:
            header += f" - é¡µç  {page}"
        parts.append(header + "\n" + d.page_content)
    return "\n\n".join(parts)


def build_rag_chain(answer_llm: ChatOpenAI):

    prompt = ChatPromptTemplate.from_messages(
        [
            ("system",
                "ä½ æ˜¯è½¦ä¼æ™ºèƒ½åº§èˆ±å¯¼èˆªå›¢é˜Ÿçš„å†…éƒ¨çŸ¥è¯†åº“åŠ©æ‰‹ã€‚\n"
                "ã€éå¸¸é‡è¦ã€‘\n"
                "1. åªèƒ½ä¾æ®â€œä¸Šä¸‹æ–‡â€ä¸­çš„å†…å®¹å›ç­”ï¼Œä¸è¦åŠ å…¥ä»»ä½•ä¸Šä¸‹æ–‡ä¹‹å¤–çš„æ¨æµ‹ã€‚\n"
                "2. å¿…é¡»ç´§æ‰£ç”¨æˆ·é—®é¢˜ä½œç­”ï¼Œä¸è¦è¾“å‡ºä¸é—®é¢˜æ— å…³çš„è§£é‡Šæˆ–èƒŒæ™¯ã€‚\n"
                "3. å¦‚æœé—®é¢˜åœ¨ä¸Šä¸‹æ–‡ä¸­æœ‰æ˜ç¡®çš„æšä¸¾æˆ–åˆ—è¡¨ï¼Œè¯·å°½é‡å®Œæ•´åˆ—å‡ºï¼Œä¸è¦éšæ„çœç•¥ã€‚\n"
                "4. å¦‚æœä¸Šä¸‹æ–‡æ²¡æœ‰è¶³å¤Ÿä¿¡æ¯ï¼Œè¯·å›ç­”â€œæ ¹æ®å½“å‰æ–‡æ¡£ä¿¡æ¯æ— æ³•ç¡®å®šâ€ã€‚\n\n"
                "ä¸‹é¢æ˜¯æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ï¼š\n{context}"),
            ("human", "{question}"),
        ]
    )

    rag_chain = (
        {
            "question": RunnablePassthrough(),
            "context": lambda x: x["context"],
        }
        | prompt
        | answer_llm
        | StrOutputParser()
    )

    return rag_chain


def build_hybrid_retriever(vectorstore: FAISS, fetch_k: int):
    dense_retriever = vectorstore.as_retriever(
        search_type="similarity",
        search_kwargs={"k": fetch_k},
    )

    all_docs = list(getattr(vectorstore.docstore, "_dict", {}).values())
    if not all_docs:
        raise ValueError("å‘é‡åº“ docstore ä¸ºç©ºï¼Œæ— æ³•æ„å»º BM25 æ£€ç´¢å™¨ã€‚")

    bm25_retriever = BM25Retriever.from_documents(all_docs)
    bm25_retriever.k = fetch_k

    weights = [DENSE_WEIGHT, BM25_WEIGHT]
    return EnsembleRetriever(
        retrievers=[dense_retriever, bm25_retriever],
        weights=weights
    )


def load_eval_items(path: Path):
    if not path.exists():
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°è¯„ä¼°é›†æ–‡ä»¶: {path}")

    items = []
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            if line.strip():
                items.append(json.loads(line))
    return items


def run_hybrid_evaluation(
    eval_items: List[dict],
    vectorstore: FAISS,
    rewrite_llm: ChatOpenAI,
    rag_chain,
    answer_llm: ChatOpenAI,
    ragas_embeddings,
):
    fetch_k = max(TOP_K, TOP_K * CANDIDATE_MULTIPLIER)
    retriever = build_hybrid_retriever(vectorstore, fetch_k)
    reranker = CrossEncoderReranker() if USE_RERANKER else None
    rewrite_cache: Dict[str, str] = {}

    print(f"\n{'=' * 15} å¼€å§‹è¯„ä¼°æ¨¡å¼ï¼šHYBRID {'=' * 15}")
    print(f"ğŸ¯ å€™é€‰æ•°é‡: {fetch_k} | æœ€ç»ˆ top-k: {TOP_K} | reranker: {'å¼€å¯' if reranker else 'å…³é—­'}")
    print(f"âš–ï¸ Denseæƒé‡: {DENSE_WEIGHT} | BM25æƒé‡: {BM25_WEIGHT}")

    questions: List[str] = []
    answers: List[str] = []
    contexts_list: List[List[str]] = []
    ground_truths: List[str] = []
    difficulties: List[str] = []
    ids: List[str] = []
    source_hints: List[str] = []

    for idx, item in enumerate(eval_items, start=1):
        qid = item.get('id', f"Q{idx}")
        question = item['question']
        gt = item['ground_truth']
        difficulty = item.get('difficulty', 'unknown')
        source_hint = item.get('source_hint', '')

        print(f"\n----- [{idx}/{len(eval_items)}] {qid} (hybrid) -----")
        print(f"â“ é—®é¢˜ï¼š{question}")
        print(f"ğŸ¯ éš¾åº¦ï¼š{difficulty} | æ¥æºæç¤ºï¼š{source_hint}")

        # Query Rewrite
        if SKIP_QUERY_REWRITE:
            rewritten = question
            print("âœï¸ å·²ç¦ç”¨ Query Rewriteï¼Œç›´æ¥ä½¿ç”¨åŸé—®é¢˜ã€‚")
        else:
            if question in rewrite_cache:
                rewritten = rewrite_cache[question]
                print(f"â™»ï¸ ä½¿ç”¨ç¼“å­˜æ”¹å†™ï¼š{rewritten}")
            else:
                rewritten = rewrite_query(question, rewrite_llm)
                rewrite_cache[question] = rewritten
                print(f"âœï¸ æ–°æ”¹å†™ï¼š{rewritten}")

        # æ£€ç´¢
        docs = retriever.invoke(rewritten)
        if not docs:
            print("âš ï¸ æœªæ£€ç´¢åˆ°ä»»ä½•æ–‡æ¡£ï¼Œä¸Šä¸‹æ–‡ç•™ç©ºã€‚")
            filtered_docs: List[Document] = []
        else:
            if reranker:
                filtered_docs = reranker.rerank(question, docs, top_k=TOP_K)
            else:
                filtered_docs = docs[:TOP_K]
            print(f"ğŸ” æ£€ç´¢å€™é€‰ {len(docs)} -> é€‰å– {len(filtered_docs)} æ¡ç”¨äºå›ç­”ã€‚")

        ctx_texts = [d.page_content for d in filtered_docs]
        prompt_context = format_docs_for_prompt(filtered_docs) if filtered_docs else ''

        # ç”Ÿæˆç­”æ¡ˆ
        answer = rag_chain.invoke(
            {
                'question': question,
                'context': prompt_context,
            }
        )
        print(f"ğŸ’¬ å›ç­”ï¼š{answer[:200]}{'...' if len(answer) > 200 else ''}")

        ids.append(qid)
        questions.append(question)
        answers.append(answer)
        contexts_list.append(ctx_texts)
        ground_truths.append(gt[0] if isinstance(gt, list) else gt)
        difficulties.append(difficulty)
        source_hints.append(source_hint)

    print("\nğŸ“Š æ„å»º RAGAS æ•°æ®é›†...")
    ds = Dataset.from_dict(
        {
            'id': ids,
            'question': questions,
            'answer': answers,
            'contexts': contexts_list,
            'ground_truth': ground_truths,
            'difficulty': difficulties,
            'source_hint': source_hints,
        }
    )

    print("âœ… æ•°æ®é›†æ„å»ºå®Œæˆï¼Œè°ƒç”¨ RAGAS è¯„ä¼°...\n")

    results = evaluate(
        ds,
        metrics=[
            context_precision,
            context_recall,
            faithfulness,
            answer_relevancy,
            answer_correctness,
        ],
        llm=answer_llm,
        embeddings=ragas_embeddings,
    )

    out_path = BASE_DIR / "ragas_results_hybrid.csv"
    df = results.to_pandas()
    df.to_csv(out_path, index=False, encoding='utf-8-sig')
    print(f"ğŸ“ˆ æ··åˆæ£€ç´¢è¯„ä¼°å®Œæˆï¼Œç»“æœå†™å…¥ {out_path}\n")

    # æ‰“å°æ±‡æ€»æŒ‡æ ‡
    print("\n====== æ±‡æ€»æŒ‡æ ‡ ======")
    for col in RAGAS_METRIC_COLUMNS:
        if col in df.columns:
            mean_val = df[col].mean()
            print(f"{col}: {mean_val:.3f}")

    return df, out_path


def main():
    print(f"ğŸ“„ æ­£åœ¨åŠ è½½è¯„ä¼°é›†: {EVAL_FILE}")
    eval_items = load_eval_items(EVAL_FILE)
    print(f"âœ… æ ·æœ¬æ•°é‡: {len(eval_items)}")

    print("ğŸ“¦ åŠ è½½å‘é‡åº“å’Œæ¨¡å‹...")
    vectorstore = load_vectorstore()
    rewrite_llm, answer_llm = get_llms()
    rag_chain = build_rag_chain(answer_llm)
    ragas_embeddings = get_embeddings()

    df, out_path = run_hybrid_evaluation(
        eval_items=eval_items,
        vectorstore=vectorstore,
        rewrite_llm=rewrite_llm,
        rag_chain=rag_chain,
        answer_llm=answer_llm,
        ragas_embeddings=ragas_embeddings,
    )

    print(f"\nâœ¨ å®Œæˆï¼ç»“æœå·²ä¿å­˜è‡³: {out_path}")


if __name__ == '__main__':
    main()

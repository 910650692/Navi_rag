# app.py
import os
import time
import uuid
from pathlib import Path
from typing import List

import streamlit as st
from dotenv import load_dotenv

from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_openai import ChatOpenAI

# æ³¨æ„è¿™é‡Œï¼šä» src.embeddings å¯¼å…¥ä½ ä¹‹å‰å†™å¥½çš„ get_embeddings()
from src.embeddings import get_embeddings
from src.reranker import CrossEncoderReranker
from src.rag_logger import get_rag_logger
from src.query_classifier import classify_and_get_strategy

# è¯»å– .envï¼ˆdeepseek çš„ key / base_urlï¼‰
load_dotenv(override=True)

# å°è¯•å¯¼å…¥ BM25 (ç”¨äºæ··åˆæ£€ç´¢)
try:
    from rank_bm25 import BM25Okapi
    import jieba
    BM25_AVAILABLE = True
except ImportError:
    BM25_AVAILABLE = False
    print("âš ï¸ rank_bm25 æˆ– jieba æœªå®‰è£…ï¼Œæ··åˆæ£€ç´¢ä¸å¯ç”¨ã€‚å®‰è£…: pip install rank-bm25 jieba")


# ========== ç¼“å­˜ä¸€äº›é‡èµ„æºï¼šå‘é‡åº“ã€æ¨¡å‹ ==========

@st.cache_resource
def load_vectorstore():
    """
    åŠ è½½å·²ç»æ„å»ºå¥½çš„ FAISS å‘é‡åº“ï¼Œå¹¶æå–æ‰€æœ‰æ–‡æ¡£ï¼ˆç”¨äºBM25ï¼‰
    è¿”å›: (vectorstore, all_docs)
    """
    base_dir = Path(__file__).parent
    index_path = base_dir / "data" / "index" / "nav_faiss"

    if not index_path.exists():
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°å‘é‡åº“ç›®å½•: {index_path}")

    embeddings = get_embeddings()
    vectorstore = FAISS.load_local(
        str(index_path),
        embeddings,
        allow_dangerous_deserialization=True,  # æœ¬åœ°ç¯å¢ƒ OK
    )

    # æå–æ‰€æœ‰æ–‡æ¡£ï¼ˆç”¨äºBM25æ··åˆæ£€ç´¢ï¼‰
    all_docs = []
    if hasattr(vectorstore, 'docstore') and hasattr(vectorstore.docstore, '_dict'):
        all_docs = list(vectorstore.docstore._dict.values())

    # è·å–ç´¢å¼•ç»Ÿè®¡ä¿¡æ¯
    total_chunks = vectorstore.index.ntotal if hasattr(vectorstore, 'index') else len(all_docs)
    print(f"ğŸ“š åŠ è½½äº† FAISS å‘é‡åº“ï¼Œå…± {total_chunks} ä¸ªchunks")

    return vectorstore, all_docs


@st.cache_resource
def get_llms():
    """è¿”å›ç”¨äºæ”¹å†™å’Œå›ç­”çš„ä¸¤ä¸ª LLM å®ä¾‹"""
    api_key = os.getenv("OPENAI_API_KEY")
    base_url = os.getenv("OPENAI_API_BASE")

    # ç”¨äº query rewriteï¼ˆéæµå¼ã€æ¸©åº¦ 0ï¼Œæ›´ç¨³å®šï¼‰
    rewrite_llm = ChatOpenAI(
        model="deepseek-chat",
        api_key=api_key,
        base_url=base_url,
        temperature=0.0,
        streaming=False,
    )

    # ç”¨äºæœ€ç»ˆå›ç­”ï¼ˆæµå¼ï¼‰
    answer_llm = ChatOpenAI(
        model="deepseek-chat",
        api_key=api_key,
        base_url=base_url,
        temperature=0.3,
        streaming=True,
    )

    return rewrite_llm, answer_llm


@st.cache_resource
def get_reranker():
    """åŠ è½½ CrossEncoder é‡æ’æ¨¡å‹ï¼ˆåªåŠ è½½ä¸€æ¬¡ï¼‰"""
    try:
        return CrossEncoderReranker()
    except Exception as e:
        st.warning(f"âš ï¸ Reranker åŠ è½½å¤±è´¥: {e}ï¼Œå°†ä¸ä½¿ç”¨é‡æ’")
        return None


# ========== Parent å†…å®¹æ‰©å±• ==========
def expand_with_parent_content(docs: List[Document], all_docs: List[Document]) -> List[Document]:
    """
    å¯¹æ£€ç´¢ç»“æœè¿›è¡Œçˆ¶èŠ‚ç‚¹å†…å®¹æ‰©å±•

    Args:
        docs: æ£€ç´¢åˆ°çš„æ–‡æ¡£åˆ—è¡¨
        all_docs: å…¨éƒ¨æ–‡æ¡£ï¼ˆç”¨äºæŸ¥æ‰¾çˆ¶èŠ‚ç‚¹ï¼‰

    Returns:
        æ‰©å±•åçš„æ–‡æ¡£åˆ—è¡¨ï¼ˆå»é‡åå¯èƒ½å˜å°‘ï¼‰
    """
    if not docs or not all_docs:
        return docs

    expanded = []
    seen_parents = set()  # é˜²æ­¢åŒä¸€çˆ¶èŠ‚ç‚¹è¢«é‡å¤æ·»åŠ 

    for doc in docs:
        parent_path = doc.metadata.get("parent_section", "")

        # å¦‚æœæ²¡æœ‰çˆ¶èŠ‚ç‚¹ï¼ˆå·²æ˜¯æ ¹èŠ‚ç‚¹ï¼‰ï¼Œç›´æ¥ä¿ç•™
        if not parent_path:
            expanded.append(doc)
            continue

        # å¦‚æœè¿™ä¸ªçˆ¶èŠ‚ç‚¹å·²ç»å¤„ç†è¿‡ï¼Œè·³è¿‡ï¼ˆé¿å…é‡å¤ï¼‰
        if parent_path in seen_parents:
            continue

        seen_parents.add(parent_path)

        # æ‰¾åˆ°çˆ¶èŠ‚ç‚¹çš„æ‰€æœ‰ chunksï¼ˆæ ¹æ® breadcrumb åŒ¹é…ï¼‰
        parent_chunks = [
            d for d in all_docs
            if d.metadata.get("breadcrumb") == parent_path
        ]

        if not parent_chunks:
            # æ‰¾ä¸åˆ°çˆ¶èŠ‚ç‚¹ï¼Œä¿ç•™åŸ chunk
            expanded.append(doc)
            continue

        # æŒ‰ global_chunk_index æ’åºï¼ˆä¿è¯é¡ºåºï¼‰
        parent_chunks.sort(key=lambda x: x.metadata.get("global_chunk_index", 0))

        # åˆå¹¶çˆ¶èŠ‚ç‚¹çš„æ‰€æœ‰å†…å®¹
        merged_content = "\n\n".join([p.page_content for p in parent_chunks])

        # åˆ›å»ºæ‰©å±•åçš„æ–‡æ¡£ï¼ˆmetadata ä¿ç•™åŸå§‹æ£€ç´¢ chunk çš„ä¿¡æ¯ï¼‰
        expanded_doc = Document(
            page_content=merged_content,
            metadata={
                **doc.metadata,
                "expanded_from": doc.metadata.get("breadcrumb", ""),  # è®°å½•åŸå§‹æ£€ç´¢æ¥æº
                "expansion_type": "parent_merge",
                "original_content_length": len(doc.page_content),
                "expanded_content_length": len(merged_content),
            }
        )

        expanded.append(expanded_doc)

    return expanded


# ========== æ··åˆæ£€ç´¢ ==========
def hybrid_retrieve(
    query: str,
    vectorstore: FAISS,
    all_docs: List[Document] = None,
    top_k: int = 10,
    dense_weight: float = 0.6,
    bm25_weight: float = 0.4,
) -> List[Document]:
    """
    æ··åˆæ£€ç´¢ï¼šDense (FAISS) + BM25

    Args:
        query: æ£€ç´¢query
        vectorstore: FAISSå‘é‡åº“
        all_docs: å…¨éƒ¨æ–‡æ¡£åˆ—è¡¨ï¼ˆç”¨äºBM25ï¼Œå¦‚æœä¸ºNoneåˆ™åªç”¨Denseï¼‰
        top_k: è¿”å›å‰kæ¡
        dense_weight: Denseæƒé‡
        bm25_weight: BM25æƒé‡

    Returns:
        èåˆåçš„Top Kæ–‡æ¡£
    """
    # Dense æ£€ç´¢
    dense_retriever = vectorstore.as_retriever(
        search_type="similarity",
        search_kwargs={"k": top_k * 3},  # å¤šå¬å›ä¸€äº›ä½œä¸ºå€™é€‰
    )
    dense_docs = dense_retriever.invoke(query)

    # å¦‚æœæ²¡æœ‰ BM25 æˆ–æ²¡æœ‰æ–‡æ¡£åˆ—è¡¨ï¼Œåªè¿”å›Denseç»“æœ
    if not BM25_AVAILABLE or not all_docs:
        return dense_docs[:top_k]

    # BM25 æ£€ç´¢
    import jieba

    # åˆ†è¯
    tokenized_corpus = [list(jieba.cut(doc.page_content)) for doc in all_docs]
    tokenized_query = list(jieba.cut(query))

    # æ„å»ºBM25ç´¢å¼•
    bm25 = BM25Okapi(tokenized_corpus)

    # è®¡ç®—BM25åˆ†æ•°
    bm25_scores = bm25.get_scores(tokenized_query)

    # è·å–Top Nçš„ç´¢å¼•
    top_n_indices = sorted(range(len(bm25_scores)), key=lambda i: bm25_scores[i], reverse=True)[:top_k * 3]
    bm25_docs = [all_docs[i] for i in top_n_indices]

    # èåˆï¼šè®¡ç®—åŠ æƒåˆ†æ•°
    doc_scores = {}

    # Dense åˆ†æ•°ï¼ˆç”¨æ’åçš„å€’æ•°ä½œä¸ºåˆ†æ•°ï¼‰
    for rank, doc in enumerate(dense_docs, 1):
        # ç”¨å†…å®¹ä½œä¸ºå”¯ä¸€æ ‡è¯†ï¼ˆæ›´å¯é ï¼‰
        doc_key = doc.page_content[:100]
        doc_scores[doc_key] = {
            'doc': doc,
            'score': dense_weight * (1.0 / rank),
        }

    # BM25 åˆ†æ•°
    for rank, doc in enumerate(bm25_docs, 1):
        doc_key = doc.page_content[:100]
        if doc_key in doc_scores:
            doc_scores[doc_key]['score'] += bm25_weight * (1.0 / rank)
        else:
            doc_scores[doc_key] = {
                'doc': doc,
                'score': bm25_weight * (1.0 / rank),
            }

    # æŒ‰åˆ†æ•°æ’åº
    sorted_docs = sorted(doc_scores.values(), key=lambda x: x['score'], reverse=True)

    # è¿”å›Top K
    return [item['doc'] for item in sorted_docs[:top_k]]


# ========== Query Rewrite ==========
def rewrite_query(question: str, rewrite_llm: ChatOpenAI) -> str:
    """
    ä½¿ç”¨ LLM æŠŠç”¨æˆ·é—®é¢˜æ”¹å†™æˆé€‚åˆæ£€ç´¢çš„å…³é”®è¯ queryã€‚
    """
    prompt = ChatPromptTemplate.from_template(
        "ä½ æ˜¯ä¸€ä¸ªæ£€ç´¢åŠ©æ‰‹ï¼Œè¯·å°†ä¸‹é¢çš„ç”¨æˆ·é—®é¢˜æ”¹å†™æˆé€‚åˆåœ¨æŠ€æœ¯æ–‡æ¡£ä¸­æ£€ç´¢çš„ç®€çŸ­æŸ¥è¯¢è¯­å¥ï¼Œ"
        "ä¿ç•™å…³é”®ä¿¡æ¯ï¼Œå¯ä»¥é€‚å½“åŠ å…¥å¯èƒ½çš„åŒä¹‰è¯æˆ–ä¸“ä¸šæœ¯è¯­ï¼Œä¸è¦å®¢å¥—è¯ï¼Œç›´æ¥è¾“å‡ºæ”¹å†™ç»“æœï¼š\n\n"
        "ç”¨æˆ·é—®é¢˜ï¼š{question}"
    )
    chain = prompt | rewrite_llm | StrOutputParser()
    rewritten = chain.invoke({"question": question})
    return rewritten.strip()


# ========== RAG Pipeline ==========
def build_rag_chain(answer_llm: ChatOpenAI):
    """
    æ„å»º RAG LCEL ç®¡é“ï¼š
    è¾“å…¥: {"question": åŸå§‹é—®é¢˜, "context": [Document, ...]}
    è¾“å‡º: ç­”æ¡ˆå­—ç¬¦ä¸²ï¼ˆé€šè¿‡ .stream() æµå¼ç”Ÿæˆï¼‰
    """
    prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                "ä½ æ˜¯è½¦ä¼æ™ºèƒ½åº§èˆ±å¯¼èˆªå›¢é˜Ÿçš„å†…éƒ¨çŸ¥è¯†åº“åŠ©æ‰‹ï¼Œ"
                "è¯·ä¸¥æ ¼æ ¹æ®æä¾›çš„ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ï¼›"
                "å¦‚æœä¸Šä¸‹æ–‡é‡Œæ²¡æœ‰ç­”æ¡ˆï¼Œå°±ç›´è¯´ä¸çŸ¥é“ï¼Œä¸è¦çç¼–ã€‚\n\n"
                "ä¸Šä¸‹æ–‡ï¼š\n{context}",
            ),
            ("human", "{question}"),
        ]
    )

    rag_chain = (
        {
            "question": lambda x: x["question"],
            "context": lambda x: format_docs(x["context"]),  # æŠŠ docs è½¬æˆå­—ç¬¦ä¸²ç»™ prompt
        }
        | prompt
        | answer_llm
        | StrOutputParser()
    )

    return rag_chain


def format_docs(docs: List[Document]) -> str:
    """æŠŠå¤šä¸ª Document æ‹¼æˆä¸€ä¸ªå¤§å­—ç¬¦ä¸²å–‚ç»™ LLM"""
    parts = []
    for i, d in enumerate(docs, 1):
        source = d.metadata.get("source", "æœªçŸ¥æ–‡ä»¶")
        page = d.metadata.get("page", None)
        section = d.metadata.get("section", None)

        header = f"[{i}] {source}"
        if section:
            header += f" - {section}"
        elif page is not None:
            header += f" - é¡µç  {page}"

        parts.append(header + "\n" + d.page_content)
    return "\n\n".join(parts)


# ========== Streamlit UI ==========

def main():
    st.set_page_config(
        page_title="å¯¼èˆªçŸ¥è¯†åº“åŠ©æ‰‹ï¼ˆRAGï¼‰",
        page_icon="ğŸ§­",
        layout="wide",
    )

    st.title("ğŸ§­ å¯¼èˆªçŸ¥è¯†åº“åŠ©æ‰‹ï¼ˆRAG v0.1ï¼‰")
    st.markdown(
        "åŸºäºå›¢é˜Ÿå†…éƒ¨æ–‡æ¡£ï¼ˆPDF / Wordï¼‰æ„å»ºçš„æœ¬åœ° RAG é—®ç­”ç³»ç»Ÿï¼Œç”¨äºæ”¯æŒæ™ºèƒ½åº§èˆ±å¯¼èˆªä¸šåŠ¡çŸ¥è¯†æŸ¥è¯¢ã€‚"
    )

    # åˆå§‹åŒ–session state
    if "history" not in st.session_state:
        st.session_state["history"] = []

    if "session_id" not in st.session_state:
        # ç”Ÿæˆå”¯ä¸€çš„session IDï¼ˆç”¨äºæ—¥å¿—è¿½è¸ªï¼‰
        st.session_state["session_id"] = str(uuid.uuid4())[:8]

    # å·¦å³å¸ƒå±€ï¼šå·¦ä¾§é—®ç­”ï¼Œå³ä¾§æ˜¾ç¤ºæ¥æº
    left_col, right_col = st.columns([2, 1])

    with left_col:
        st.subheader("ğŸ’¬ æé—®")
        question = st.text_area(
            "è¯·è¾“å…¥ä½ çš„é—®é¢˜ï¼š",
            placeholder="ä¾‹å¦‚ï¼šé«˜å¾·åœ°å›¾æ¨åŒ…æµç¨‹æ˜¯ä»€ä¹ˆï¼Ÿ / ä»£ç æäº¤æµç¨‹æ˜¯æ€æ ·çš„ï¼Ÿ",
            height=100,
        )

        # æ£€ç´¢è®¾ç½®
        col1, col2, col3, col4 = st.columns([1, 1, 1, 1])
        with col1:
            search_type = st.selectbox(
                "ğŸ” æ£€ç´¢æ–¹å¼",
                ["Dense", "Hybrid"] if BM25_AVAILABLE else ["Dense"],
                help="Dense: çº¯å‘é‡æ£€ç´¢ | Hybrid: å‘é‡+BM25æ··åˆ"
            )
        with col2:
            use_rewrite = st.checkbox("ğŸ”„ Queryæ”¹å†™", value=True, help="ä½¿ç”¨LLMæ”¹å†™é—®é¢˜ä¸ºæ£€ç´¢å…³é”®è¯")
        with col3:
            use_reranker = st.checkbox("âœ¨ é‡æ’åº", value=True, help="ä½¿ç”¨CrossEncoderé‡æ’åº")
        with col4:
            top_k = st.number_input("è¿”å›æ–‡æ¡£æ•°", min_value=3, max_value=10, value=6, help="æœ€ç»ˆè¿”å›çš„æ–‡æ¡£æ•°é‡")

        if st.button("å‘é€", type="primary"):
            # åˆå§‹åŒ–å˜é‡
            start_time = time.time()
            retrieval_start = None
            retrieval_latency = None
            llm_start = None
            llm_latency = None
            error_info = None
            error_type = None
            rewritten_query = None
            docs = []
            full_answer = ""
            strategy = None  # ä¿å­˜ç­–ç•¥ä¿¡æ¯

            try:
                if not question.strip():
                    st.warning("è¯·å…ˆè¾“å…¥é—®é¢˜ã€‚")
                    return

                # 0. Query åˆ†ç±»ä¸ç­–ç•¥é€‰æ‹©ï¼ˆAdaptive RAGï¼‰
                with st.spinner("ğŸ¤– åˆ†æé—®é¢˜ç±»å‹..."):
                    strategy = classify_and_get_strategy(question)

                # æ˜¾ç¤ºåˆ†ç±»ç»“æœ
                classification = strategy.get("classification", {})
                category = classification.get("category", "unknown")
                confidence = classification.get("confidence", 0)
                reasoning = classification.get("reasoning", "")

                st.info(f"""
**ğŸ§  Queryåˆ†ç±»ç»“æœ**
- **ç±»å‹**: `{category}`
- **ç½®ä¿¡åº¦**: {confidence:.2f}
- **ç†ç”±**: {reasoning}
                """)

                # æ ¹æ®ç­–ç•¥å†³å®šæ˜¯å¦è·³è¿‡æ£€ç´¢
                if strategy.get("skip_retrieval", False):
                    # no_retrieval: ç›´æ¥LLMå›ç­”ï¼Œä¸æ£€ç´¢
                    st.caption("ğŸ’¬ æ­¤é—®é¢˜æ— éœ€æ£€ç´¢çŸ¥è¯†åº“ï¼Œç›´æ¥å›ç­”")

                    # åŠ è½½LLM
                    with st.spinner("åŠ è½½æ¨¡å‹ä¸­..."):
                        _, answer_llm = get_llms()

                    # ç›´æ¥ç”Ÿæˆç­”æ¡ˆ
                    st.subheader("ğŸ§  å›ç­”")
                    answer_placeholder = st.empty()
                    full_answer = ""

                    # ä½¿ç”¨ç®€å•çš„å¯¹è¯prompt
                    from langchain_core.prompts import ChatPromptTemplate
                    simple_prompt = ChatPromptTemplate.from_messages([
                        ("system", "ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„åŠ©æ‰‹ã€‚"),
                        ("human", "{question}")
                    ])

                    simple_chain = simple_prompt | answer_llm | StrOutputParser()

                    llm_start = time.time()
                    for chunk in simple_chain.stream({"question": question}):
                        full_answer += chunk
                        answer_placeholder.markdown(full_answer)

                    llm_latency = (time.time() - llm_start) * 1000

                    # ä¿å­˜åˆ°å¯¹è¯å†å²
                    st.session_state["history"].append(
                        {"question": question, "answer": full_answer, "sources": []}
                    )

                    # è·³è¿‡åç»­æ£€ç´¢æµç¨‹
                    return

                # å¦‚æœéœ€è¦æ£€ç´¢ï¼Œç»§ç»­åŸæœ‰æµç¨‹
                # åŠ è½½èµ„æº
                with st.spinner("åŠ è½½å‘é‡åº“å’Œæ¨¡å‹ä¸­..."):
                    vectorstore, all_docs = load_vectorstore()
                    rewrite_llm, answer_llm = get_llms()

                # æ ¹æ®ç­–ç•¥è¦†ç›–éƒ¨åˆ†ç”¨æˆ·è®¾ç½®
                adaptive_top_k = strategy.get("top_k", top_k)
                adaptive_use_reranker = strategy.get("use_reranker", use_reranker)
                adaptive_retrieval_mode = strategy.get("retrieval_mode", "dense")

                st.caption(f"""
**ğŸ¯ Adaptive RAG ç­–ç•¥**
æ£€ç´¢æ–¹å¼: {adaptive_retrieval_mode.upper()} | Top-K: {adaptive_top_k} | Queryæ”¹å†™: {'âœ“' if use_rewrite else 'âœ—'} (ç”¨æˆ·è®¾ç½®) | Reranker: {'âœ“' if adaptive_use_reranker else 'âœ—'}
                """)

                # 1. Query Rewrite (ç”±ç”¨æˆ·å‹¾é€‰å†³å®š)
                if use_rewrite:
                    with st.spinner("æ­£åœ¨æ”¹å†™æ£€ç´¢ Query..."):
                        rewritten_query = rewrite_query(question, rewrite_llm)
                    st.write(f"âœï¸ **æ£€ç´¢ç”¨æ”¹å†™ï¼š** `{rewritten_query}`")
                else:
                    rewritten_query = question
                    st.write(f"âœï¸ **ä½¿ç”¨åŸé—®é¢˜æ£€ç´¢**")

                # 2. æ£€ç´¢æ–‡æ¡£ï¼ˆæ ¹æ®ç­–ç•¥é€‰æ‹©æ£€ç´¢æ–¹å¼ï¼‰
                retrieval_start = time.time()
                candidate_k = strategy.get("candidate_k", 20)

                with st.spinner("æ­£åœ¨æ£€ç´¢ç›¸å…³æ–‡æ¡£..."):
                    if adaptive_retrieval_mode == "hybrid" and BM25_AVAILABLE:
                        # æ··åˆæ£€ç´¢ï¼šDense + BM25
                        candidate_docs = hybrid_retrieve(
                            query=rewritten_query,
                            vectorstore=vectorstore,
                            all_docs=all_docs,
                            top_k=candidate_k,
                            dense_weight=0.6,
                            bm25_weight=0.4
                        )
                        st.caption(f"ğŸ” ä½¿ç”¨ Hybrid æ£€ç´¢ï¼ˆå¬å› {len(candidate_docs)} æ¡å€™é€‰ï¼‰")
                    else:
                        # Denseæ£€ç´¢
                        retriever = vectorstore.as_retriever(
                            search_type="similarity",
                            search_kwargs={"k": candidate_k},
                        )
                        candidate_docs = retriever.invoke(rewritten_query)
                        st.caption(f"ğŸ” ä½¿ç”¨ Dense æ£€ç´¢ï¼ˆå¬å› {len(candidate_docs)} æ¡å€™é€‰ï¼‰")

                if not candidate_docs:
                    st.error("æ²¡æœ‰æ£€ç´¢åˆ°ç›¸å…³æ–‡æ¡£ï¼Œå¯èƒ½è¯­æ–™é‡Œè¿˜æ²¡æœ‰ç›¸å…³å†…å®¹ã€‚")
                    return

                # 3. CrossEncoder é‡æ’åº (æ ¹æ®ç­–ç•¥å†³å®š)
                if adaptive_use_reranker:
                    reranker = get_reranker()
                    if reranker:
                        with st.spinner("æ­£åœ¨é‡æ’åºæ–‡æ¡£..."):
                            docs = reranker.rerank(rewritten_query, candidate_docs, top_k=adaptive_top_k)
                        st.caption(f"âœ¨ CrossEncoder ç²¾æ’ï¼Œè¿”å› Top {len(docs)}")
                    else:
                        docs = candidate_docs[:adaptive_top_k]
                        st.caption(f"âš ï¸ RerankeræœªåŠ è½½ï¼Œç›´æ¥è¿”å› Top {adaptive_top_k}")
                else:
                    docs = candidate_docs[:adaptive_top_k]
                    st.caption(f"ğŸ“‹ ç›´æ¥è¿”å› Top {adaptive_top_k}")

                # 4. Parent å†…å®¹æ‰©å±• (æ ¹æ®ç­–ç•¥å†³å®š)
                if strategy.get("expand_context", False):
                    with st.spinner("æ­£åœ¨æ‰©å±•çˆ¶èŠ‚ç‚¹ä¸Šä¸‹æ–‡..."):
                        original_count = len(docs)
                        docs = expand_with_parent_content(docs, all_docs)
                        st.caption(f"ğŸŒ³ Parent æ‰©å±•ï¼š{original_count} â†’ {len(docs)} ä¸ªæ–‡æ¡£ï¼ˆå»é‡åï¼‰")

                retrieval_latency = (time.time() - retrieval_start) * 1000  # è½¬ä¸ºæ¯«ç§’

                # 5. æ„å»º RAG é“¾ï¼Œæµå¼ç”Ÿæˆç­”æ¡ˆ
                st.subheader("ğŸ§  å›ç­”")

                answer_placeholder = st.empty()
                full_answer = ""

                rag_chain = build_rag_chain(answer_llm)

                llm_start = time.time()
                with st.spinner("æ­£åœ¨ç”Ÿæˆå›ç­”..."):
                    for chunk in rag_chain.stream({"question": question, "context": docs}):
                        full_answer += chunk
                        answer_placeholder.markdown(full_answer)

                llm_latency = (time.time() - llm_start) * 1000  # è½¬ä¸ºæ¯«ç§’

                # ä¿å­˜åˆ°å¯¹è¯å†å²
                st.session_state["history"].append(
                    {"question": question, "answer": full_answer, "sources": docs}
                )

            except Exception as e:
                error_info = str(e)
                error_type = type(e).__name__
                st.error(f"âŒ å‘ç”Ÿé”™è¯¯: {error_info}")

            finally:
                # è®°å½•æ—¥å¿—ï¼ˆåŒ…å«åˆ†ç±»å’Œç­–ç•¥ä¿¡æ¯ï¼‰
                total_latency = (time.time() - start_time) * 1000
                logger = get_rag_logger()

                # æ„å»ºæ—¥å¿—æ•°æ®
                log_data = {
                    "session_id": st.session_state["session_id"],
                    "query": question,
                    "rewritten_query": rewritten_query,
                    "use_rewriter": use_rewrite,
                    "use_reranker": use_reranker,
                    "use_hybrid": (search_type == "Hybrid") if 'search_type' in locals() else False,
                    "top_k": top_k,
                    "retrieval_docs": docs,
                    "answer": full_answer,
                    "latency_ms": total_latency,
                    "retrieval_latency_ms": retrieval_latency,
                    "llm_latency_ms": llm_latency,
                    "error": error_info,
                    "error_type": error_type,
                }

                # æ·»åŠ Adaptive RAGä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
                if strategy:
                    log_data["query_classification"] = strategy.get("classification", {})
                    log_data["adaptive_strategy"] = {
                        "category": strategy.get("classification", {}).get("category"),
                        "skip_retrieval": strategy.get("skip_retrieval", False),
                        "retrieval_mode": strategy.get("retrieval_mode"),
                        "top_k": strategy.get("top_k"),
                        "use_reranker": strategy.get("use_reranker"),
                    }

                logger.log_query(**log_data)

                # åœ¨UIä¸Šæ˜¾ç¤ºæ—¥å¿—è·¯å¾„ï¼ˆä»…å¼€å‘æ—¶ï¼‰
                if error_info is None:
                    st.caption(f"ğŸ“ æŸ¥è¯¢å·²è®°å½•åˆ°æ—¥å¿— | Session: {st.session_state['session_id']} | è€—æ—¶: {total_latency:.0f}ms")

        # æ˜¾ç¤ºå†å²å¯¹è¯ï¼ˆåœ¨æŒ‰é’®å¤–éƒ¨ï¼Œé¿å…ç‚¹å‡»å…¶ä»–ç»„ä»¶æ—¶æ¶ˆå¤±ï¼‰
        if "history" in st.session_state and st.session_state["history"]:
            with st.expander("ğŸ“ æœ€è¿‘ä¸€æ¬¡é—®ç­”", expanded=False):
                last_turn = st.session_state["history"][-1]

                st.markdown("**é—®é¢˜ï¼š**")
                st.info(last_turn["question"])

                st.markdown("**å›ç­”ï¼š**")
                st.markdown(last_turn["answer"])

    # å³ä¾§ï¼šæ˜¾ç¤ºæ¥æº
    with right_col:
        st.subheader("ğŸ“˜ æœ¬æ¬¡æ£€ç´¢åˆ°çš„æ–‡æ¡£ç‰‡æ®µ")

        if "history" in st.session_state and st.session_state["history"]:
            last_turn = st.session_state["history"][-1]
            docs = last_turn["sources"]

            for i, d in enumerate(docs, 1):
                source = d.metadata.get("source", "æœªçŸ¥æ–‡ä»¶")
                page = d.metadata.get("page", None)
                section = d.metadata.get("section", None)
                doc_type = d.metadata.get("doc_type", "")

                # ä¼˜å…ˆæ˜¾ç¤ºsectionï¼Œå…¶æ¬¡æ˜¯page
                location_str = ""
                if section:
                    location_str = f" - {section}"
                elif page is not None:
                    location_str = f" - é¡µç  {page}"

                snippet = d.page_content[:200].replace("\n", " ")

                with st.expander(f"[{i}] {source}{location_str}", expanded=(i == 1)):
                    if doc_type:
                        st.caption(f"ç±»å‹: {doc_type}")
                    st.write(snippet + ("..." if len(d.page_content) > 200 else ""))
        else:
            st.info("æäº¤é—®é¢˜åï¼Œè¿™é‡Œä¼šæ˜¾ç¤ºç›¸å…³æ–‡æ¡£æ¥æºã€‚")


if __name__ == "__main__":
    main()

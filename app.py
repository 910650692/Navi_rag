# app.py
import os
from pathlib import Path
from typing import List

import streamlit as st
from dotenv import load_dotenv

from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_openai import ChatOpenAI

# æ³¨æ„è¿™é‡Œï¼šä» src.embeddings å¯¼å…¥ä½ ä¹‹å‰å†™å¥½çš„ get_embeddings()
from src.embeddings import get_embeddings

# è¯»å– .envï¼ˆdeepseek çš„ key / base_urlï¼‰
load_dotenv(override=True)


# ========== ç¼“å­˜ä¸€äº›é‡èµ„æºï¼šå‘é‡åº“ã€æ¨¡å‹ ==========

@st.cache_resource
def load_vectorstore():
    """åŠ è½½å·²ç»æ„å»ºå¥½çš„ FAISS å‘é‡åº“ï¼ˆåªåŠ è½½ä¸€æ¬¡ï¼Œåé¢å¤ç”¨ï¼‰"""
    base_dir = Path(__file__).parent
    index_path = base_dir / "data" / "index" / "nav_faiss"

    if not index_path.exists():
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°å‘é‡åº“ç›®å½•: {index_path}")

    embeddings = get_embeddings()
    vectorstore = FAISS.load_local(
        str(index_path),
        embeddings,
        allow_dangerous_deserialization=True,  # æœ¬åœ°ç¯å¢ƒ OK
    )
    return vectorstore


@st.cache_resource
def get_llms():
    """è¿”å›ç”¨äºæ”¹å†™å’Œå›ç­”çš„ä¸¤ä¸ª LLM å®ä¾‹"""
    api_key = os.getenv("OPENAI_API_KEY")
    base_url = os.getenv("OPENAI_API_BASE")

    # ç”¨äº query rewriteï¼ˆéæµå¼ã€æ¸©åº¦ 0ï¼Œæ›´ç¨³å®šï¼‰
    rewrite_llm = ChatOpenAI(
        model="deepseek-chat",
        api_key=api_key,
        base_url=base_url,
        temperature=0.0,
        streaming=False,
    )

    # ç”¨äºæœ€ç»ˆå›ç­”ï¼ˆæµå¼ï¼‰
    answer_llm = ChatOpenAI(
        model="deepseek-chat",
        api_key=api_key,
        base_url=base_url,
        temperature=0.3,
        streaming=True,
    )

    return rewrite_llm, answer_llm


# ========== Query Rewrite ==========
def rewrite_query(question: str, rewrite_llm: ChatOpenAI) -> str:
    """
    ä½¿ç”¨ LLM æŠŠç”¨æˆ·é—®é¢˜æ”¹å†™æˆé€‚åˆæ£€ç´¢çš„å…³é”®è¯ queryã€‚
    """
    prompt = ChatPromptTemplate.from_template(
        "ä½ æ˜¯ä¸€ä¸ªæ£€ç´¢åŠ©æ‰‹ï¼Œè¯·å°†ä¸‹é¢çš„ç”¨æˆ·é—®é¢˜æ”¹å†™æˆé€‚åˆåœ¨æŠ€æœ¯æ–‡æ¡£ä¸­æ£€ç´¢çš„ç®€çŸ­æŸ¥è¯¢è¯­å¥ï¼Œ"
        "ä¿ç•™å…³é”®ä¿¡æ¯ï¼Œå¯ä»¥é€‚å½“åŠ å…¥å¯èƒ½çš„åŒä¹‰è¯æˆ–ä¸“ä¸šæœ¯è¯­ï¼Œä¸è¦å®¢å¥—è¯ï¼Œç›´æ¥è¾“å‡ºæ”¹å†™ç»“æœï¼š\n\n"
        "ç”¨æˆ·é—®é¢˜ï¼š{question}"
    )
    chain = prompt | rewrite_llm | StrOutputParser()
    rewritten = chain.invoke({"question": question})
    return rewritten.strip()


# ========== RAG Pipeline ==========
def build_rag_chain(answer_llm: ChatOpenAI):
    """
    æ„å»º RAG LCEL ç®¡é“ï¼š
    è¾“å…¥: {"question": åŸå§‹é—®é¢˜, "context": [Document, ...]}
    è¾“å‡º: ç­”æ¡ˆå­—ç¬¦ä¸²ï¼ˆé€šè¿‡ .stream() æµå¼ç”Ÿæˆï¼‰
    """
    prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                "ä½ æ˜¯è½¦ä¼æ™ºèƒ½åº§èˆ±å¯¼èˆªå›¢é˜Ÿçš„å†…éƒ¨çŸ¥è¯†åº“åŠ©æ‰‹ï¼Œ"
                "è¯·ä¸¥æ ¼æ ¹æ®æä¾›çš„ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ï¼›"
                "å¦‚æœä¸Šä¸‹æ–‡é‡Œæ²¡æœ‰ç­”æ¡ˆï¼Œå°±ç›´è¯´ä¸çŸ¥é“ï¼Œä¸è¦çç¼–ã€‚\n\n"
                "ä¸Šä¸‹æ–‡ï¼š\n{context}",
            ),
            ("human", "{question}"),
        ]
    )

    rag_chain = (
        {
            "question": lambda x: x["question"],
            "context": lambda x: format_docs(x["context"]),  # æŠŠ docs è½¬æˆå­—ç¬¦ä¸²ç»™ prompt
        }
        | prompt
        | answer_llm
        | StrOutputParser()
    )

    return rag_chain


def format_docs(docs: List[Document]) -> str:
    """æŠŠå¤šä¸ª Document æ‹¼æˆä¸€ä¸ªå¤§å­—ç¬¦ä¸²å–‚ç»™ LLM"""
    parts = []
    for i, d in enumerate(docs, 1):
        source = d.metadata.get("source", "æœªçŸ¥æ–‡ä»¶")
        page = d.metadata.get("page", None)
        header = f"[{i}] {source}"
        if page is not None:
            header += f" - é¡µç  {page}"
        parts.append(header + "\n" + d.page_content)
    return "\n\n".join(parts)


# ========== Streamlit UI ==========

def main():
    st.set_page_config(
        page_title="å¯¼èˆªçŸ¥è¯†åº“åŠ©æ‰‹ï¼ˆRAGï¼‰",
        page_icon="ğŸ§­",
        layout="wide",
    )

    st.title("ğŸ§­ å¯¼èˆªçŸ¥è¯†åº“åŠ©æ‰‹ï¼ˆRAG v0.1ï¼‰")
    st.markdown(
        "åŸºäºå›¢é˜Ÿå†…éƒ¨æ–‡æ¡£ï¼ˆPDF / Wordï¼‰æ„å»ºçš„æœ¬åœ° RAG é—®ç­”ç³»ç»Ÿï¼Œç”¨äºæ”¯æŒæ™ºèƒ½åº§èˆ±å¯¼èˆªä¸šåŠ¡çŸ¥è¯†æŸ¥è¯¢ã€‚"
    )

    # å·¦å³å¸ƒå±€ï¼šå·¦ä¾§é—®ç­”ï¼Œå³ä¾§æ˜¾ç¤ºæ¥æº
    left_col, right_col = st.columns([2, 1])

    with left_col:
        st.subheader("ğŸ’¬ æé—®")
        question = st.text_area(
            "è¯·è¾“å…¥ä½ çš„é—®é¢˜ï¼š",
            placeholder="ä¾‹å¦‚ï¼šé«˜å¾·åœ°å›¾æ¨åŒ…æµç¨‹æ˜¯ä»€ä¹ˆï¼Ÿ / ä»£ç æäº¤æµç¨‹æ˜¯æ€æ ·çš„ï¼Ÿ",
            height=100,
        )

        if "history" not in st.session_state:
            st.session_state["history"] = []

        if st.button("å‘é€", type="primary"):

            if not question.strip():
                st.warning("è¯·å…ˆè¾“å…¥é—®é¢˜ã€‚")
                return

            # åŠ è½½èµ„æº
            with st.spinner("åŠ è½½å‘é‡åº“å’Œæ¨¡å‹ä¸­..."):
                vectorstore = load_vectorstore()
                rewrite_llm, answer_llm = get_llms()

            # 1. Query Rewrite
            with st.spinner("æ­£åœ¨æ”¹å†™æ£€ç´¢ Query..."):
                rewritten_query = rewrite_query(question, rewrite_llm)
            st.write(f"âœï¸ **æ£€ç´¢ç”¨æ”¹å†™ï¼š** `{rewritten_query}`")

            # 2. MMR æ£€ç´¢
            with st.spinner("æ­£åœ¨æ£€ç´¢ç›¸å…³æ–‡æ¡£..."):
                retriever = vectorstore.as_retriever(
                    search_type="mmr",
                    search_kwargs={"k": 4, "fetch_k": 20},
                )
                docs = retriever.invoke(rewritten_query)

            if not docs:
                st.error("æ²¡æœ‰æ£€ç´¢åˆ°ç›¸å…³æ–‡æ¡£ï¼Œå¯èƒ½è¯­æ–™é‡Œè¿˜æ²¡æœ‰ç›¸å…³å†…å®¹ã€‚")
                return

            # 3. æ„å»º RAG é“¾ï¼Œæµå¼ç”Ÿæˆç­”æ¡ˆ
            st.subheader("ğŸ§  å›ç­”")

            answer_placeholder = st.empty()
            full_answer = ""

            rag_chain = build_rag_chain(answer_llm)

            with st.spinner("æ­£åœ¨ç”Ÿæˆå›ç­”..."):
                for chunk in rag_chain.stream({"question": question, "context": docs}):
                    full_answer += chunk
                    answer_placeholder.markdown(full_answer)

            # ä¿å­˜åˆ°å¯¹è¯å†å²
            st.session_state["history"].append(
                {"question": question, "answer": full_answer, "sources": docs}
            )

    # å³ä¾§ï¼šæ˜¾ç¤ºæ¥æº
    with right_col:
        st.subheader("ğŸ“˜ æœ¬æ¬¡æ£€ç´¢åˆ°çš„æ–‡æ¡£ç‰‡æ®µ")

        if "history" in st.session_state and st.session_state["history"]:
            last_turn = st.session_state["history"][-1]
            docs = last_turn["sources"]

            for i, d in enumerate(docs, 1):
                source = d.metadata.get("source", "æœªçŸ¥æ–‡ä»¶")
                page = d.metadata.get("page", None)
                page_str = f" - é¡µç  {page}" if page is not None else ""
                snippet = d.page_content[:200].replace("\n", " ")

                with st.expander(f"[{i}] {source}{page_str}", expanded=(i == 1)):
                    st.write(snippet + ("..." if len(d.page_content) > 200 else ""))
        else:
            st.info("æäº¤é—®é¢˜åï¼Œè¿™é‡Œä¼šæ˜¾ç¤ºç›¸å…³æ–‡æ¡£æ¥æºã€‚")


if __name__ == "__main__":
    main()
